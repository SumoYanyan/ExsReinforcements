{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Return--\n",
      "> <ipython-input-6-638e69289976>(3)<module>()->None\n",
      "-> pdb.set_trace()\n",
      "(Pdb) c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:27<00:00, 67.39it/s]\n",
      "100%|██████████| 2000/2000 [00:25<00:00, 78.23it/s]\n",
      "100%|██████████| 2000/2000 [00:28<00:00, 69.24it/s]\n",
      "100%|██████████| 2000/2000 [00:24<00:00, 83.30it/s]\n",
      "100%|██████████| 2000/2000 [00:24<00:00, 79.28it/s]\n",
      "100%|██████████| 2000/2000 [00:26<00:00, 76.90it/s]\n",
      "100%|██████████| 2000/2000 [00:01<00:00, 1059.05it/s]\n",
      "100%|██████████| 2000/2000 [00:01<00:00, 1220.43it/s]\n",
      "100%|██████████| 2000/2000 [00:09<00:00, 202.47it/s]\n",
      "100%|██████████| 2000/2000 [00:10<00:00, 184.60it/s]\n",
      "100%|██████████| 2000/2000 [00:09<00:00, 205.14it/s]\n",
      "100%|██████████| 2000/2000 [00:31<00:00, 62.88it/s]\n",
      "100%|██████████| 2000/2000 [00:34<00:00, 57.98it/s]\n",
      "100%|██████████| 2000/2000 [00:29<00:00, 67.99it/s]\n"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "import pdb\n",
    "pdb.set_trace()\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import trange\n",
    "\n",
    "matplotlib.use('Agg')\n",
    "\n",
    "\n",
    "class Bandit:\n",
    "    # @k_arm: # of arms\n",
    "    # @epsilon: probability for exploration in epsilon-greedy algorithm\n",
    "    # @initial: initial estimation for each action\n",
    "    # @step_size: constant step size for updating estimations\n",
    "    # @sample_averages: if True, use sample averages to update estimations instead of constant step size\n",
    "    # @UCB_param: if not None, use UCB algorithm to select action\n",
    "    # @gradient: if True, use gradient based bandit algorithm\n",
    "    # @gradient_baseline: if True, use average reward as baseline for gradient based bandit algorithm\n",
    "    def __init__(self, k_arm=10, epsilon=0., initial=0., step_size=0.1, sample_averages=False, \n",
    "                 true_reward=0., bias=False):\n",
    "        self.k = k_arm\n",
    "        self.step_size = step_size\n",
    "        self.sample_averages = sample_averages\n",
    "        self.bias = bias\n",
    "        self.indices = np.arange(self.k)\n",
    "        self.time = 0\n",
    "        self.true_reward = true_reward\n",
    "        self.epsilon = epsilon\n",
    "        self.initial = initial\n",
    "        \n",
    "\n",
    "    def reset(self):\n",
    "        # real reward for each action\n",
    "        self.q_true = np.random.randn(self.k) + self.true_reward # np.random.normal(0, 0.01, self.k)\n",
    "        # mean, standard deviation, num\n",
    "\n",
    "        # estimation for each action\n",
    "        self.q_estimation = np.zeros(self.k) + self.initial\n",
    "\n",
    "        # # of chosen times for each action\n",
    "        self.action_count = np.zeros(self.k)\n",
    "\n",
    "        self.best_action = np.argmax(self.q_true)\n",
    "\n",
    "        self.time = 0\n",
    "        \n",
    "        self.o = np.zeros(self.k)\n",
    "        \n",
    "        self.beta = np.zeros(self.k)\n",
    "\n",
    "    # get an action for this bandit\n",
    "    def act(self):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.choice(self.indices)\n",
    "        \n",
    "        q_best = np.max(self.q_estimation)\n",
    "        return np.random.choice(np.where(self.q_estimation == q_best)[0])\n",
    "        # 若有两个以上具有相同q_estimation\n",
    "\n",
    "    # take an action, update estimation for this action\n",
    "    def step(self, action, nonstationary):\n",
    "        # generate the reward under N(real reward, 1) with stationary\n",
    "        mean = np.random.normal(0, nonstationary)\n",
    "        reward = self.q_true[action] + mean # np.random.randn() + \n",
    "        self.time += 1\n",
    "        self.action_count[action] += 1\n",
    "        \n",
    "        if self.sample_averages:\n",
    "            # update estimation using sample averages\n",
    "            self.q_estimation[action] += (reward - self.q_estimation[action]) / self.action_count[action]\n",
    "        elif self.bias:\n",
    "            self.o[action] += self.step_size * (1 - self.o[action])\n",
    "            self.beta[action] = self.step_size / self.o[action]\n",
    "            self.q_estimation[action] += self.beta[action] * (reward - self.q_estimation[action])\n",
    "        else:\n",
    "            # update estimation with constant step size\n",
    "            self.q_estimation[action] += self.step_size * (reward - self.q_estimation[action])\n",
    "        return reward\n",
    "\n",
    "\n",
    "def simulate(runs, time, bandits, nonstationary):\n",
    "    rewards = np.zeros((len(bandits), runs, time))\n",
    "    best_action_counts = np.zeros(rewards.shape)\n",
    "    for i, bandit in enumerate(bandits):\n",
    "        for r in trange(runs):\n",
    "            bandit.reset()\n",
    "            for t in range(time):\n",
    "                action = bandit.act()\n",
    "                reward = bandit.step(action, nonstationary)\n",
    "                rewards[i, r, t] = reward\n",
    "                if action == bandit.best_action:\n",
    "                    best_action_counts[i, r, t] = 1\n",
    "    mean_best_action_counts = best_action_counts.mean(axis=1)\n",
    "    mean_rewards = rewards.mean(axis=1)\n",
    "    return mean_best_action_counts, mean_rewards\n",
    "\n",
    "def figure_2_1(runs=2000, time=800):    \n",
    "    bandits = []\n",
    "    bandits.append(Bandit(epsilon=0., bias=True, initial=5))\n",
    "    bandits.append(Bandit(epsilon=0., bias=False, initial=5))\n",
    "    nonstationary = 1\n",
    "    best_action_counts, average_rewards = simulate(runs, time, bandits, nonstationary)\n",
    "\n",
    "    plt.plot(average_rewards[0], label='bias exponential recency-weighted average')\n",
    "    plt.plot(average_rewards[1], label='exponential recency-weighted average')\n",
    "    plt.xlabel('Steps')\n",
    "    plt.ylabel('Average reward')\n",
    "    plt.legend()\n",
    "    plt.title('epsilon=0')\n",
    "\n",
    "    plt.savefig('../images/figure_2_7_1.png')\n",
    "    plt.close()\n",
    "\n",
    "    plt.plot(best_action_counts[0], label='bias exponential recency-weighted average')\n",
    "    plt.plot(best_action_counts[1], label='exponential recency-weighted average')\n",
    "    plt.xlabel('Steps')\n",
    "    plt.ylabel('% optimal action')\n",
    "    plt.legend()\n",
    "    plt.title('epsilon=0')\n",
    "\n",
    "    plt.savefig('../images/figure_2_7_2.png')\n",
    "    plt.close()\n",
    "    \n",
    "def figure_2_2(runs=2000, time=800):    \n",
    "    bandits = []\n",
    "    bandits.append(Bandit(epsilon=0.1, bias=True, initial=5))\n",
    "    bandits.append(Bandit(epsilon=0.1, bias=False, initial=5))\n",
    "    nonstationary = 1\n",
    "    best_action_counts, average_rewards = simulate(runs, time, bandits, nonstationary)\n",
    "\n",
    "    plt.plot(average_rewards[0], label='bias exponential recency-weighted average')\n",
    "    plt.plot(average_rewards[1], label='exponential recency-weighted average')\n",
    "    plt.xlabel('Steps')\n",
    "    plt.ylabel('Average reward')\n",
    "    plt.legend()\n",
    "    plt.title('epsilon=0.1')\n",
    "\n",
    "    plt.savefig('../images/figure_2_7_3.png')\n",
    "    plt.close()\n",
    "\n",
    "    plt.plot(best_action_counts[0], label='bias exponential recency-weighted average')\n",
    "    plt.plot(best_action_counts[1], label='exponential recency-weighted average')\n",
    "    plt.xlabel('Steps')\n",
    "    plt.ylabel('% optimal action')\n",
    "    plt.legend()\n",
    "    plt.title('epsilon=0.1')\n",
    "\n",
    "    plt.savefig('../images/figure_2_7_4.png')\n",
    "    plt.close() \n",
    "    \n",
    "    \n",
    "def figure_2_3(runs=2000, time=800):    \n",
    "    bandits = []\n",
    "    bandits.append(Bandit(epsilon=0.1, bias=False, initial=5))\n",
    "    bandits.append(Bandit(epsilon=0., bias=False, initial=5))\n",
    "    nonstationary = 1\n",
    "    best_action_counts, average_rewards = simulate(runs, time, bandits, nonstationary)\n",
    "\n",
    "    plt.plot(average_rewards[0], label='epsilon=0.1')\n",
    "    plt.plot(average_rewards[1], label='epsilon=0')\n",
    "    plt.xlabel('Steps')\n",
    "    plt.ylabel('Average reward')\n",
    "    plt.legend()\n",
    "    plt.title('exponential recency-weighted average')\n",
    "\n",
    "    plt.savefig('../images/figure_2_7_5.png')\n",
    "    plt.close()\n",
    "\n",
    "    plt.plot(best_action_counts[0], label='epsilon=0.1')\n",
    "    plt.plot(best_action_counts[1], label='epsilon=0')\n",
    "    plt.xlabel('Steps')\n",
    "    plt.ylabel('% optimal action')\n",
    "    plt.legend()\n",
    "    plt.title('exponential recency-weighted average')\n",
    "\n",
    "    plt.savefig('../images/figure_2_7_6.png')\n",
    "    plt.close() \n",
    "    \n",
    "def figure_2_4(runs=2000, time=50):    \n",
    "    bandits = []\n",
    "    bandits.append(Bandit(epsilon=0., bias=True, initial=50))\n",
    "    bandits.append(Bandit(epsilon=0., bias=False, initial=50))\n",
    "    nonstationary = 1\n",
    "    best_action_counts, average_rewards = simulate(runs, time, bandits, nonstationary)\n",
    "\n",
    "    plt.plot(average_rewards[0], label='bias exponential recency-weighted average')\n",
    "    plt.plot(average_rewards[1], label='exponential recency-weighted average')\n",
    "    plt.xlabel('Steps')\n",
    "    plt.ylabel('Average reward')\n",
    "    plt.legend()\n",
    "    plt.title('initial=50')\n",
    "\n",
    "    plt.savefig('../images/figure_2_7_7.png')\n",
    "    plt.close()\n",
    "\n",
    "    plt.plot(best_action_counts[0], label='bias exponential recency-weighted average')\n",
    "    plt.plot(best_action_counts[1], label='exponential recency-weighted average')\n",
    "    plt.xlabel('Steps')\n",
    "    plt.ylabel('% optimal action')\n",
    "    plt.legend()\n",
    "    plt.title('initial=50')\n",
    "\n",
    "    plt.savefig('../images/figure_2_7_8.png')\n",
    "    plt.close()\n",
    "    \n",
    "def figure_2_5(runs=2000, time=300):    \n",
    "    bandits = []\n",
    "    bandits.append(Bandit(epsilon=0., sample_averages=True, initial=0))\n",
    "    bandits.append(Bandit(epsilon=0., bias=True, initial=0))\n",
    "    bandits.append(Bandit(epsilon=0., bias=False, initial=0))\n",
    "    nonstationary = 2\n",
    "    best_action_counts, average_rewards = simulate(runs, time, bandits, nonstationary)\n",
    "\n",
    "    plt.plot(average_rewards[0], label='sample_average')\n",
    "    plt.plot(average_rewards[1], label='bias exponential recency-weighted average')\n",
    "    plt.plot(average_rewards[2], label='exponential recency-weighted average')\n",
    "    plt.xlabel('Steps')\n",
    "    plt.ylabel('Average reward')\n",
    "    plt.legend()\n",
    "    plt.title('nonstationary=2, epsilon=0')\n",
    "\n",
    "    plt.savefig('../images/figure_2_7_9.png')\n",
    "    plt.close()\n",
    "\n",
    "    plt.plot(best_action_counts[0], label='sample_average')\n",
    "    plt.plot(best_action_counts[1], label='bias exponential recency-weighted average')\n",
    "    plt.plot(best_action_counts[2], label='exponential recency-weighted average')\n",
    "    plt.xlabel('Steps')\n",
    "    plt.ylabel('% optimal action')\n",
    "    plt.legend()\n",
    "    plt.title('nonstationary=2, epsilon=0')\n",
    "\n",
    "    plt.savefig('../images/figure_2_7_10.png')\n",
    "    plt.close()\n",
    "    \n",
    "def figure_2_6(runs=2000, time=1000):    \n",
    "    bandits = []\n",
    "    bandits.append(Bandit(epsilon=0.1, sample_averages=True, initial=0))\n",
    "    bandits.append(Bandit(epsilon=0.1, bias=True, initial=0))\n",
    "    bandits.append(Bandit(epsilon=0.1, bias=False, initial=0))\n",
    "    nonstationary = 2\n",
    "    best_action_counts, average_rewards = simulate(runs, time, bandits, nonstationary)\n",
    "\n",
    "    plt.plot(average_rewards[0], label='sample_average')\n",
    "    plt.plot(average_rewards[1], label='bias exponential recency-weighted average')\n",
    "    plt.plot(average_rewards[2], label='exponential recency-weighted average')\n",
    "    plt.xlabel('Steps')\n",
    "    plt.ylabel('Average reward')\n",
    "    plt.legend()\n",
    "    plt.title('nonstationary=2, epsilon=0.1')\n",
    "\n",
    "    plt.savefig('../images/figure_2_7_11.png')\n",
    "    plt.close()\n",
    "\n",
    "    plt.plot(best_action_counts[0], label='sample_average')\n",
    "    plt.plot(best_action_counts[1], label='bias exponential recency-weighted average')\n",
    "    plt.plot(best_action_counts[2], label='exponential recency-weighted average')\n",
    "    plt.xlabel('Steps')\n",
    "    plt.ylabel('% optimal action')\n",
    "    plt.legend()\n",
    "    plt.title('nonstationary=2, epsilon=0.1')\n",
    "\n",
    "    plt.savefig('../images/figure_2_7_12.png')\n",
    "    plt.close()\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    figure_2_1()\n",
    "    figure_2_2() \n",
    "    figure_2_3()\n",
    "    figure_2_4()\n",
    "    figure_2_5()\n",
    "    figure_2_6()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
